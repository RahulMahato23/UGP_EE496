{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vision_transformer_noise_depth_kaggle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Kaggle environment...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up Kaggle environment...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set device - Kaggle provides Tesla P100 or T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: NVIDIA A30\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create output directory in Kaggle working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/home/akshy_grp12/output2', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR10 dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CIFAR10 dataset...\")\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='/home/akshy_grp12/dataset', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='/home/akshy_grp12/dataset', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation samples: 5000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "print(f\"Validation samples: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chane 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sweep grids\n",
    "noise_vals   = [round(x, 2) for x in np.linspace(0.5, 1.0, 11)]  # 0.00, 0.05, ..., 1.00\n",
    "depth_vals   = list(range(2, 18, 2))                              # 4, 6, 8, 10, 12, 14, 16\n",
    "mlp_vals     = [round(x, 2) for x in np.arange(0.5, 6.0 + 1e-9, 0.5)]  # 0.5, 1.0, ..., 6.0\n",
    "\n",
    "EPOCHS = 30  # Or your desired value; used across training and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(depth=12, embed_dim=192, num_heads=3, mlp_ratio=4.0,\n",
    "                     drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0):\n",
    "    model = timm.create_model(\n",
    "        'vit_tiny_patch16_224',\n",
    "        pretrained=False, num_classes=10,\n",
    "        img_size=32, patch_size=4,\n",
    "        embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "        qkv_bias=True, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n",
    "        drop_path_rate=drop_path_rate\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "_inv = transforms.Normalize(mean=[-m/s for m,s in zip(CIFAR_MEAN, CIFAR_STD)],\n",
    "                            std=[1/s for s in CIFAR_STD])\n",
    "_norm = transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "\n",
    "def add_noise_in_rgb_space(x, sigma: float):\n",
    "    \"\"\"x normalized (B,C,H,W) or (C,H,W); sigma in [0,1] RGB scale.\"\"\"\n",
    "    batched = (x.dim() == 4)\n",
    "    if not batched: x = x.unsqueeze(0)\n",
    "    x_rgb = torch.clamp(_inv(x), 0.0, 1.0)\n",
    "    x_noisy = torch.clamp(x_rgb + sigma * torch.randn_like(x_rgb), 0.0, 1.0)\n",
    "    x_noisy = _norm(x_noisy)\n",
    "    return x_noisy if batched else x_noisy.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vit_model(model, loader, test_noise=0.0):\n",
    "    \"\"\"Evaluate Vision Transformer on test set.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    gt_all = []\n",
    "    pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            if test_noise > 0:\n",
    "                data = add_noise_in_rgb_space(data, test_noise)\n",
    "            \n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            gt_all.extend(target.cpu().numpy())\n",
    "            pred_all.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy, gt_all, pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vit_model(model, train_loader, val_loader=None, noise_factor=0.0, epochs=30):\n",
    "    \"\"\"Train Vision Transformer\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = [] if val_loader is not None else None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for data, target in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            if noise_factor > 0:\n",
    "                data = add_noise_in_rgb_space(data, noise_factor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validation evaluation\n",
    "        if val_loader is not None:\n",
    "            val_acc = evaluate_vit_model(model, val_loader, test_noise=0.0)\n",
    "            val_accuracies.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%', end='')\n",
    "            if val_loader is not None:\n",
    "                print(f', Val Acc: {val_accuracies[-1]:.2f}%')\n",
    "            else:\n",
    "                print()\n",
    "    \n",
    "    return train_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_v2(embed_dim=192, num_heads=3, drop_path_rate=0.1):\n",
    "    \"\"\"Main experiment with depth Ã— mlp_ratio per noise, logs params & accuracy.\"\"\"\n",
    "    import time\n",
    "    print(\"Starting ViT sweepâ€¦ (depth, mlp_ratio) per noise\")\n",
    "\n",
    "    rows = []\n",
    "    seen_param_for = {}  # (depth, mlp) -> params\n",
    "\n",
    "    for noise in noise_vals:\n",
    "        print(f\"\\n{'='*20} train_noise={noise} {'='*20}\")\n",
    "        for depth in depth_vals:\n",
    "            for mlp in mlp_vals:\n",
    "                # Build model\n",
    "                model = create_vit_model(depth=depth, embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, mlp_ratio=mlp,\n",
    "                                         drop_path_rate=drop_path_rate)\n",
    "\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                seen_param_for.setdefault((depth, mlp), total_params)\n",
    "\n",
    "                # if (PARAM_BUDGET is not None) and (total_params > PARAM_BUDGET):\n",
    "                #     print(f\"Skip depth={depth}, mlp={mlp} ({total_params/1e6:.1f}M > budget)\")\n",
    "                #     del model; torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                #     continue\n",
    "\n",
    "                print(f\"depth={depth:>2}, mlp={mlp:>3}, params={total_params/1e6:.2f}M, noise={noise}\")\n",
    "\n",
    "                start = time.time()\n",
    "                train_losses, train_accuracies, _ = train_vit_model(\n",
    "                    model, \n",
    "                    train_loader, \n",
    "                    val_loader=None, \n",
    "                    noise_factor=noise, \n",
    "                    epochs=EPOCHS\n",
    "                )\n",
    "                train_time = time.time() - start\n",
    "\n",
    "                acc_clean = evaluate_vit_model(model, test_loader, test_noise=0.0)\n",
    "                rows.append({\n",
    "                    \"train_noise\": noise,\n",
    "                    \"depth\": depth,\n",
    "                    \"mlp_ratio\": mlp,\n",
    "                    \"embed_dim\": embed_dim,\n",
    "                    \"num_heads\": num_heads,\n",
    "                    \"params\": total_params,\n",
    "                    \"epochs\": EPOCHS,\n",
    "                    \"test_noise\": 0.0,\n",
    "                    \"accuracy\": acc_clean,\n",
    "                    \"train_time_s\": round(train_time, 2)\n",
    "                })\n",
    "\n",
    "                # Save checkpoint\n",
    "                torch.save(model.state_dict(),\n",
    "                           f'/home/akshy_grp12/output2/vit_d{depth}_mlp{mlp}_noise{noise}.pth')\n",
    "\n",
    "                del model\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"/home/akshy_grp12/output2/vit_grid_results.csv\", index=False)\n",
    "    print(\"Saved /home/akshy_grp12/output2/vit_grid_results.csv\")\n",
    "    return df, seen_param_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(results, depths, noise_factors, model_params):\n",
    "    \"\"\"Plot and save results\"\"\"\n",
    "    # Line plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(depths)))\n",
    "    \n",
    "    for i, depth in enumerate(depths):\n",
    "        plt.plot(noise_factors, results[depth], \n",
    "                marker='o', linewidth=2, markersize=6, \n",
    "                color=colors[i], label=f'Depth {depth}')\n",
    "    \n",
    "    plt.xlabel('Noise Factor')\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.title('ViT: Accuracy vs Noise for Different Depths')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('/home/akshy_grp12/output2/vit_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results, index=noise_factors)\n",
    "    results_df.index.name = 'Noise_Factor'\n",
    "    results_df.to_csv('/home/akshy_grp12/output2/vit_results.csv')\n",
    "    \n",
    "    # Save parameters info\n",
    "    params_df = pd.DataFrame(list(model_params.items()), columns=['Depth', 'Parameters'])\n",
    "    params_df.to_csv('/home/akshy_grp12/output2/model_parameters.csv', index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluation that optionally adds test-time noise (keep default clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vit_model(model, test_loader, test_noise=0.0):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if test_noise > 0:\n",
    "                data = add_noise_in_rgb_space(data, test_noise)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            pred = logits.argmax(1)\n",
    "            total += target.size(0)\n",
    "            correct += (pred == target).sum().item()\n",
    "    return 100.0 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started at: 2025-11-13 08:10:13.228056\n",
      "Starting ViT sweepâ€¦ (depth, mlp_ratio) per noise\n",
      "\n",
      "==================== train_noise=0.5 ====================\n",
      "depth= 2, mlp=0.5, params=0.40M, noise=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 1.8142, Train Acc: 33.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 1.7099, Train Acc: 37.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Loss: 1.6512, Train Acc: 39.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 1.6047, Train Acc: 41.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Loss: 1.5708, Train Acc: 42.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Loss: 1.5555, Train Acc: 43.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= 2, mlp=1.0, params=0.47M, noise=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 1.8090, Train Acc: 33.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 1.7067, Train Acc: 37.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Loss: 1.6446, Train Acc: 39.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 1.5914, Train Acc: 41.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Loss: 1.5556, Train Acc: 43.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Loss: 1.5375, Train Acc: 43.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= 2, mlp=1.5, params=0.54M, noise=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 1.8174, Train Acc: 33.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 274/391 [00:06<00:02, 40.63it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Experiment started at: {start_time}\")\n",
    "    \n",
    "    results_df, model_params = run_experiment_v2()  # Assumes it returns df and params dict\n",
    "    \n",
    "    # Extract full sweep data for plotting\n",
    "    depths = sorted(results_df['depth'].unique())  # e.g., [2,4,...,16]\n",
    "    noise_factors = sorted(results_df['train_noise'].unique())  # Full [0.0, 0.05, ..., 1.0]\n",
    "    \n",
    "    # Reshape results to {depth: [accs for sorted noise_factors]}\n",
    "    results = {}\n",
    "    for depth in depths:\n",
    "        depth_data = results_df[results_df['depth'] == depth].sort_values('train_noise')\n",
    "        results[depth] = depth_data['accuracy'].values.tolist()\n",
    "    \n",
    "    # Plot and save (now uses full sweep)\n",
    "    plot_and_save_results(results, depths, noise_factors, model_params)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(f\"Experiment completed at: {end_time}\")\n",
    "    print(f\"Total duration: {end_time - start_time}\")\n",
    "    \n",
    "    # Print full summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(results_df.groupby(['depth', 'train_noise'])['accuracy'].mean().unstack())  # Pivot table for viz\n",
    "    \n",
    "    # Metadata with full details\n",
    "    metadata = {\n",
    "        'experiment_date': start_time.isoformat(),\n",
    "        'duration_seconds': (end_time - start_time).total_seconds(),\n",
    "        'depths_tested': list(depths),\n",
    "        'noise_factors': noise_factors,\n",
    "        'epochs_per_model': 30,  # Or your EPOCHS\n",
    "        'results': {str(k): dict(zip(noise_factors, v)) for k, v in results.items()},\n",
    "        'model_parameters': model_params\n",
    "    }\n",
    "    \n",
    "    with open('/home/akshy_grp12/output2/experiment_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"\\nAll results saved to /home/akshy_grp12/output2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T05:24:28.203001Z",
     "iopub.status.idle": "2025-10-16T05:24:28.203365Z",
     "shell.execute_reply": "2025-10-16T05:24:28.203185Z",
     "shell.execute_reply.started": "2025-10-16T05:24:28.203170Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set style for better plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T05:24:28.205153Z",
     "iopub.status.idle": "2025-10-16T05:24:28.205452Z",
     "shell.execute_reply": "2025-10-16T05:24:28.205337Z",
     "shell.execute_reply.started": "2025-10-16T05:24:28.205322Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T05:24:28.206375Z",
     "iopub.status.idle": "2025-10-16T05:24:28.206617Z",
     "shell.execute_reply": "2025-10-16T05:24:28.206514Z",
     "shell.execute_reply.started": "2025-10-16T05:24:28.206503Z"
    }
   },
   "outputs": [],
   "source": [
    "class ViTExperimentAnalyzer:\n",
    "    def __init__(self, results_dir='/kaggle/working/outputs/'):\n",
    "        self.results_dir = results_dir\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all experiment results\"\"\"\n",
    "        try:\n",
    "            # Load results CSV\n",
    "            self.results_df = pd.read_csv(f'{self.results_dir}vit_results.csv', index_col='Noise_Factor')\n",
    "            \n",
    "            # Load model parameters\n",
    "            self.params_df = pd.read_csv(f'{self.results_dir}model_parameters.csv')\n",
    "            \n",
    "            # Load metadata\n",
    "            with open(f'{self.results_dir}experiment_metadata.json', 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "                \n",
    "            print(\"âœ… Data loaded successfully!\")\n",
    "            print(f\"Experiment date: {self.metadata['experiment_date']}\")\n",
    "            print(f\"Duration: {self.metadata['duration_seconds']/60:.1f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading data: {e}\")\n",
    "            # Create sample data for demonstration\n",
    "            self.create_sample_data()\n",
    "    \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Create sample data if real data isn't available\"\"\"\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        depths = [4, 6, 8]\n",
    "        noise_factors = [0.0, 0.1, 0.2, 0.3]\n",
    "        \n",
    "        # Sample results\n",
    "        self.results_df = pd.DataFrame({\n",
    "            '4': [85.2, 78.5, 72.1, 65.3],\n",
    "            '6': [87.5, 82.1, 76.8, 68.9],\n",
    "            '8': [88.9, 84.2, 79.5, 72.1]\n",
    "        }, index=noise_factors)\n",
    "        self.results_df.index.name = 'Noise_Factor'\n",
    "        \n",
    "        # Sample parameters\n",
    "        self.params_df = pd.DataFrame({\n",
    "            'Depth': [4, 6, 8],\n",
    "            'Parameters': [3850000, 5650000, 7450000]\n",
    "        })\n",
    "        \n",
    "        self.metadata = {\n",
    "            'experiment_date': datetime.now().isoformat(),\n",
    "            'duration_seconds': 7200,\n",
    "            'depths_tested': depths,\n",
    "            'noise_factors': noise_factors,\n",
    "            'epochs_per_model': 30\n",
    "        }\n",
    "    \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Print basic statistics about the experiment\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BASIC EXPERIMENT STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Depths tested: {self.metadata['depths_tested']}\")\n",
    "        print(f\"Noise factors: {self.metadata['noise_factors']}\")\n",
    "        print(f\"Epochs per model: {self.metadata['epochs_per_model']}\")\n",
    "        \n",
    "        print(\"\\nModel Parameters:\")\n",
    "        for _, row in self.params_df.iterrows():\n",
    "            print(f\"  Depth {row['Depth']}: {row['Parameters']:,} parameters\")\n",
    "        \n",
    "        print(\"\\nPerformance Summary:\")\n",
    "        print(self.results_df.describe())\n",
    "    \n",
    "    def plot_comprehensive_analysis(self):\n",
    "        \"\"\"Create comprehensive visualization of results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Vision Transformer: Comprehensive Analysis of Depth vs Noise', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Main accuracy vs noise plot\n",
    "        self._plot_accuracy_vs_noise(axes[0, 0])\n",
    "        \n",
    "        # 2. Performance heatmap\n",
    "        self._plot_accuracy_heatmap(axes[0, 1])\n",
    "        \n",
    "        # 3. Performance drop analysis\n",
    "        self._plot_performance_drop(axes[0, 2])\n",
    "        \n",
    "        # 4. Parameter efficiency\n",
    "        self._plot_parameter_efficiency(axes[1, 0])\n",
    "        \n",
    "        # 5. Noise robustness ranking\n",
    "        self._plot_robustness_ranking(axes[1, 1])\n",
    "        \n",
    "        # 6. Optimal depth analysis\n",
    "        self._plot_optimal_depth(axes[1, 2])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.results_dir}comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_accuracy_vs_noise(self, ax):\n",
    "        \"\"\"Plot accuracy vs noise for different depths\"\"\"\n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        noise_factors = self.results_df.index\n",
    "        \n",
    "        for depth in depths:\n",
    "            ax.plot(noise_factors, self.results_df[str(depth)], \n",
    "                   marker='o', linewidth=2.5, markersize=8, label=f'Depth {depth}')\n",
    "        \n",
    "        ax.set_xlabel('Noise Factor')\n",
    "        ax.set_ylabel('Test Accuracy (%)')\n",
    "        ax.set_title('Accuracy vs Noise Level\\nby Transformer Depth')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_accuracy_heatmap(self, ax):\n",
    "        \"\"\"Plot accuracy heatmap\"\"\"\n",
    "        accuracy_matrix = self.results_df.values\n",
    "        im = ax.imshow(accuracy_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Accuracy (%)')\n",
    "        ax.set_xlabel('Transformer Depth')\n",
    "        ax.set_ylabel('Noise Factor')\n",
    "        ax.set_title('Accuracy Heatmap:\\nDepth vs Noise')\n",
    "        \n",
    "        # Set ticks\n",
    "        ax.set_xticks(range(len(self.results_df.columns)))\n",
    "        ax.set_xticklabels([f'Depth {d}' for d in self.results_df.columns.astype(int)])\n",
    "        ax.set_yticks(range(len(self.results_df.index)))\n",
    "        ax.set_yticklabels(self.results_df.index)\n",
    "        \n",
    "        # Add accuracy values\n",
    "        for i in range(len(self.results_df.index)):\n",
    "            for j in range(len(self.results_df.columns)):\n",
    "                ax.text(j, i, f'{accuracy_matrix[i, j]:.1f}%',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "    \n",
    "    def _plot_performance_drop(self, ax):\n",
    "        \"\"\"Plot performance drop from clean to noisy data\"\"\"\n",
    "        performance_drops = []\n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        \n",
    "        for depth in depths:\n",
    "            clean_acc = self.results_df[str(depth)].iloc[0]  # Noise 0.0\n",
    "            noisy_acc = self.results_df[str(depth)].iloc[-1]  # Highest noise\n",
    "            performance_drop = clean_acc - noisy_acc\n",
    "            performance_drops.append(performance_drop)\n",
    "        \n",
    "        bars = ax.bar(range(len(depths)), performance_drops, \n",
    "                     color=plt.cm.viridis(np.linspace(0, 1, len(depths))))\n",
    "        \n",
    "        ax.set_xlabel('Transformer Depth')\n",
    "        ax.set_ylabel('Accuracy Drop (%)')\n",
    "        ax.set_title('Performance Drop:\\nClean â†’ High Noise ({self.results_df.index[-1]})')\n",
    "        ax.set_xticks(range(len(depths)))\n",
    "        ax.set_xticklabels([f'Depth {d}' for d in depths])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, drop in zip(bars, performance_drops):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                   f'{drop:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    def _plot_parameter_efficiency(self, ax):\n",
    "        \"\"\"Plot parameter efficiency analysis\"\"\"\n",
    "        efficiency_scores = []\n",
    "        depths = self.params_df['Depth'].values\n",
    "        \n",
    "        for depth in depths:\n",
    "            avg_accuracy = np.mean(self.results_df[str(depth)])\n",
    "            params_million = self.params_df[self.params_df['Depth'] == depth]['Parameters'].values[0] / 1e6\n",
    "            efficiency = avg_accuracy / params_million\n",
    "            efficiency_scores.append(efficiency)\n",
    "        \n",
    "        bars = ax.bar(range(len(depths)), efficiency_scores,\n",
    "                     color=plt.cm.plasma(np.linspace(0, 1, len(depths))))\n",
    "        \n",
    "        ax.set_xlabel('Transformer Depth')\n",
    "        ax.set_ylabel('Efficiency (Accuracy / Million Params)')\n",
    "        ax.set_title('Parameter Efficiency')\n",
    "        ax.set_xticks(range(len(depths)))\n",
    "        ax.set_xticklabels([f'Depth {d}' for d in depths])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, eff in zip(bars, efficiency_scores):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{eff:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    def _plot_robustness_ranking(self, ax):\n",
    "        \"\"\"Plot noise robustness ranking\"\"\"\n",
    "        robustness_scores = {}\n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        \n",
    "        for depth in depths:\n",
    "            clean_acc = self.results_df[str(depth)].iloc[0]\n",
    "            high_noise_acc = self.results_df[str(depth)].iloc[-1]\n",
    "            robustness = (high_noise_acc / clean_acc) * 100\n",
    "            robustness_scores[depth] = robustness\n",
    "        \n",
    "        # Sort by robustness\n",
    "        sorted_depths = sorted(robustness_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        depths_sorted, robustness_sorted = zip(*sorted_depths)\n",
    "        \n",
    "        bars = ax.bar(range(len(depths_sorted)), robustness_sorted,\n",
    "                     color=plt.cm.coolwarm(np.linspace(0, 1, len(depths_sorted))))\n",
    "        \n",
    "        ax.set_xlabel('Transformer Depth')\n",
    "        ax.set_ylabel('Robustness (% of Clean Accuracy Retained)')\n",
    "        ax.set_title('Noise Robustness Ranking')\n",
    "        ax.set_xticks(range(len(depths_sorted)))\n",
    "        ax.set_xticklabels([f'Depth {d}' for d in depths_sorted])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, robustness in zip(bars, robustness_sorted):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f'{robustness:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    def _plot_optimal_depth(self, ax):\n",
    "        \"\"\"Plot optimal depth analysis for different noise levels\"\"\"\n",
    "        noise_levels = self.results_df.index\n",
    "        optimal_depths = []\n",
    "        \n",
    "        for noise in noise_levels:\n",
    "            accuracies = [self.results_df[str(depth)][noise] for depth in self.results_df.columns.astype(int)]\n",
    "            optimal_depth = self.results_df.columns.astype(int)[np.argmax(accuracies)]\n",
    "            optimal_depths.append(optimal_depth)\n",
    "        \n",
    "        ax.plot(noise_levels, optimal_depths, marker='s', linewidth=3, markersize=10, color='red')\n",
    "        ax.set_xlabel('Noise Factor')\n",
    "        ax.set_ylabel('Optimal Depth')\n",
    "        ax.set_title('Optimal Transformer Depth\\nby Noise Level')\n",
    "        ax.set_yticks(self.results_df.columns.astype(int))\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (noise, depth) in enumerate(zip(noise_levels, optimal_depths)):\n",
    "            ax.annotate(f'Depth {depth}', (noise, depth), \n",
    "                       textcoords=\"offset points\", xytext=(0,10), ha='center', fontweight='bold')\n",
    "    \n",
    "    def advanced_metrics_analysis(self):\n",
    "        \"\"\"Perform advanced metrics analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ADVANCED METRICS ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        \n",
    "        # 1. Best performing models\n",
    "        print(\"\\nðŸ† BEST PERFORMING MODELS:\")\n",
    "        best_clean = self.results_df.iloc[0].idxmax()\n",
    "        best_clean_acc = self.results_df.iloc[0].max()\n",
    "        print(f\"  Clean data (noise=0.0): Depth {best_clean} - {best_clean_acc:.2f}%\")\n",
    "        \n",
    "        best_noisy = self.results_df.iloc[-1].idxmax()\n",
    "        best_noisy_acc = self.results_df.iloc[-1].max()\n",
    "        # print(f\"  Noisy data (noise=0.3): Depth {best_noisy} - {best_noisy_acc:.2f}%\")\n",
    "        print(f\"  Noisy data (noise={self.results_df.index[-1]}): Depth {best_noisy} - {best_noisy_acc:.2f}%\")\n",
    "        \n",
    "        best_avg = self.results_df.mean().idxmax()\n",
    "        best_avg_acc = self.results_df.mean().max()\n",
    "        print(f\"  Average all noise levels: Depth {best_avg} - {best_avg_acc:.2f}%\")\n",
    "        \n",
    "        # 2. Robustness analysis\n",
    "        print(\"\\nðŸ›¡ï¸ NOISE ROBUSTNESS ANALYSIS:\")\n",
    "        robustness_data = []\n",
    "        for depth in depths:\n",
    "            clean_acc = self.results_df[str(depth)].iloc[0]\n",
    "            noisy_acc = self.results_df[str(depth)].iloc[-1]\n",
    "            robustness = (noisy_acc / clean_acc) * 100\n",
    "            drop = clean_acc - noisy_acc\n",
    "            robustness_data.append({\n",
    "                'Depth': depth,\n",
    "                'Clean_Acc': clean_acc,\n",
    "                'Noisy_Acc': noisy_acc,\n",
    "                'Robustness_%': robustness,\n",
    "                'Absolute_Drop': drop\n",
    "            })\n",
    "        \n",
    "        robustness_df = pd.DataFrame(robustness_data)\n",
    "        print(robustness_df.round(2))\n",
    "        \n",
    "        # 3. Parameter efficiency\n",
    "        print(\"\\nâš¡ PARAMETER EFFICIENCY:\")\n",
    "        efficiency_data = []\n",
    "        for depth in depths:\n",
    "            avg_acc = np.mean(self.results_df[str(depth)])\n",
    "            params = self.params_df[self.params_df['Depth'] == depth]['Parameters'].values[0]\n",
    "            params_million = params / 1e6\n",
    "            efficiency = avg_acc / params_million\n",
    "            \n",
    "            efficiency_data.append({\n",
    "                'Depth': depth,\n",
    "                'Avg_Accuracy': avg_acc,\n",
    "                'Params(M)': params_million,\n",
    "                'Efficiency': efficiency\n",
    "            })\n",
    "        \n",
    "        efficiency_df = pd.DataFrame(efficiency_data)\n",
    "        print(efficiency_df.round(3))\n",
    "        \n",
    "        # 4. Depth scaling analysis\n",
    "        print(\"\\nðŸ“ˆ DEPTH SCALING ANALYSIS:\")\n",
    "        for i in range(1, len(depths)):\n",
    "            prev_depth = depths[i-1]\n",
    "            curr_depth = depths[i]\n",
    "            \n",
    "            clean_improvement = self.results_df[str(curr_depth)].iloc[0] - self.results_df[str(prev_depth)].iloc[0]\n",
    "            avg_improvement = np.mean(self.results_df[str(curr_depth)]) - np.mean(self.results_df[str(prev_depth)])\n",
    "            \n",
    "            prev_params = self.params_df[self.params_df['Depth'] == prev_depth]['Parameters'].values[0]\n",
    "            curr_params = self.params_df[self.params_df['Depth'] == curr_depth]['Parameters'].values[0]\n",
    "            param_increase_pct = ((curr_params - prev_params) / prev_params) * 100\n",
    "            \n",
    "            print(f\"  Depth {prev_depth}â†’{curr_depth}:\")\n",
    "            print(f\"    Clean Acc: +{clean_improvement:.2f}%\")\n",
    "            print(f\"    Avg Acc: +{avg_improvement:.2f}%\")\n",
    "            print(f\"    Params: +{param_increase_pct:.1f}%\")\n",
    "    \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate practical recommendations based on analysis\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PRACTICAL RECOMMENDATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        \n",
    "        # Find best models for different scenarios\n",
    "        best_clean_depth = self.results_df.iloc[0].idxmax()\n",
    "        best_noisy_depth = self.results_df.iloc[-1].idxmax()\n",
    "        best_avg_depth = self.results_df.mean().idxmax()\n",
    "        \n",
    "        # Robustness ranking\n",
    "        robustness_scores = {}\n",
    "        for depth in depths:\n",
    "            clean_acc = self.results_df[str(depth)].iloc[0]\n",
    "            noisy_acc = self.results_df[str(depth)].iloc[-1]\n",
    "            robustness_scores[depth] = (noisy_acc / clean_acc) * 100\n",
    "        \n",
    "        most_robust_depth = max(robustness_scores, key=robustness_scores.get)\n",
    "        \n",
    "        # Efficiency ranking\n",
    "        efficiency_scores = {}\n",
    "        for depth in depths:\n",
    "            avg_acc = np.mean(self.results_df[str(depth)])\n",
    "            params = self.params_df[self.params_df['Depth'] == depth]['Parameters'].values[0]\n",
    "            efficiency_scores[depth] = avg_acc / (params / 1e6)\n",
    "        \n",
    "        most_efficient_depth = max(efficiency_scores, key=efficiency_scores.get)\n",
    "        \n",
    "        print(\"ðŸŽ¯ RECOMMENDED CONFIGURATIONS:\")\n",
    "        print(f\"  â€¢ Clean Data Environment: Depth {best_clean_depth}\")\n",
    "        print(f\"  â€¢ Noisy Data Environment: Depth {best_noisy_depth}\")\n",
    "        print(f\"  â€¢ Balanced Performance: Depth {best_avg_depth}\")\n",
    "        print(f\"  â€¢ Maximum Robustness: Depth {most_robust_depth}\")\n",
    "        print(f\"  â€¢ Computational Efficiency: Depth {most_efficient_depth}\")\n",
    "        \n",
    "        print(\"\\nðŸ“Š KEY INSIGHTS:\")\n",
    "        \n",
    "        # Check if deeper is better\n",
    "        depth_trend = self.results_df.iloc[0].values[-1] - self.results_df.iloc[0].values[0]\n",
    "        if depth_trend > 2:\n",
    "            print(\"  âœ“ Deeper models generally perform better on clean data\")\n",
    "        elif depth_trend < -2:\n",
    "            print(\"  âœ— Deeper models may overfit on clean data\")\n",
    "        else:\n",
    "            print(\"  âž¡ Depth has minimal impact on clean data performance\")\n",
    "        \n",
    "        # Robustness trend\n",
    "        robustness_trend = robustness_scores[depths[-1]] - robustness_scores[depths[0]]\n",
    "        if robustness_trend > 5:\n",
    "            print(\"  âœ“ Deeper models are more robust to noise\")\n",
    "        elif robustness_trend < -5:\n",
    "            print(\"  âœ— Shallower models are more robust to noise\")\n",
    "        else:\n",
    "            print(\"  âž¡ Depth has minimal impact on noise robustness\")\n",
    "        \n",
    "        # Efficiency insight\n",
    "        if most_efficient_depth == depths[0]:\n",
    "            print(\"  ðŸ’¡ Shallower models offer best computational efficiency\")\n",
    "        elif most_efficient_depth == depths[-1]:\n",
    "            print(\"  ðŸ’¡ Deeper models provide good performance per parameter\")\n",
    "        else:\n",
    "            print(f\"  ðŸ’¡ Depth {most_efficient_depth} offers the best efficiency trade-off\")\n",
    "    \n",
    "    def save_analysis_report(self):\n",
    "        \"\"\"Save comprehensive analysis report\"\"\"\n",
    "        report = {\n",
    "            'analysis_timestamp': datetime.now().isoformat(),\n",
    "            'experiment_metadata': self.metadata,\n",
    "            'summary_statistics': {\n",
    "                'best_clean_accuracy': float(self.results_df.iloc[0].max()),\n",
    "                'best_noisy_accuracy': float(self.results_df.iloc[-1].max()),\n",
    "                'average_accuracy_by_depth': {depth: float(np.mean(self.results_df[str(depth)])) \n",
    "                                            for depth in self.results_df.columns.astype(int)},\n",
    "                'performance_drops': {depth: float(self.results_df[str(depth)].iloc[0] - self.results_df[str(depth)].iloc[-1])\n",
    "                                    for depth in self.results_df.columns.astype(int)}\n",
    "            },\n",
    "            'recommendations': self._generate_recommendations_dict()\n",
    "        }\n",
    "        \n",
    "        with open(f'{self.results_dir}analysis_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nðŸ“„ Analysis report saved to: {self.results_dir}analysis_report.json\")\n",
    "    \n",
    "    def _generate_recommendations_dict(self):\n",
    "        \"\"\"Generate recommendations as dictionary\"\"\"\n",
    "        depths = self.results_df.columns.astype(int)\n",
    "        \n",
    "        best_clean_depth = int(self.results_df.iloc[0].idxmax())\n",
    "        best_noisy_depth = int(self.results_df.iloc[-1].idxmax())\n",
    "        best_avg_depth = int(self.results_df.mean().idxmax())\n",
    "        \n",
    "        return {\n",
    "            'best_for_clean_data': best_clean_depth,\n",
    "            'best_for_noisy_data': best_noisy_depth,\n",
    "            'best_overall_balance': best_avg_depth,\n",
    "            'computational_budget_low': int(depths[0]),\n",
    "            'computational_budget_high': int(depths[-1])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T05:24:28.207985Z",
     "iopub.status.idle": "2025-10-16T05:24:28.208585Z",
     "shell.execute_reply": "2025-10-16T05:24:28.208399Z",
     "shell.execute_reply.started": "2025-10-16T05:24:28.208382Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ” Starting Vision Transformer Experiment Analysis...\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = ViTExperimentAnalyzer()\n",
    "    \n",
    "    # Run analyses\n",
    "    analyzer.basic_statistics()\n",
    "    analyzer.plot_comprehensive_analysis()\n",
    "    analyzer.advanced_metrics_analysis()\n",
    "    analyzer.generate_recommendations()\n",
    "    analyzer.save_analysis_report()\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Analysis completed successfully!\")\n",
    "    print(\"Check the output directory for comprehensive analysis plots and reports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T05:24:28.209861Z",
     "iopub.status.idle": "2025-10-16T05:24:28.210203Z",
     "shell.execute_reply": "2025-10-16T05:24:28.210048Z",
     "shell.execute_reply.started": "2025-10-16T05:24:28.210033Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "df = pd.read_csv(\"vit_grid_results.csv\")\n",
    "\n",
    "# Keep only evaluations on clean test for the surface (test_noise==0)\n",
    "df = df[df[\"test_noise\"] == 0.0].copy()\n",
    "\n",
    "def plot_surface_for_noise(df_noise, noise_value):\n",
    "    # Create mesh grid\n",
    "    depths_sorted = sorted(df_noise[\"depth\"].unique())\n",
    "    mlps_sorted   = sorted(df_noise[\"mlp_ratio\"].unique())\n",
    "    D, M = np.meshgrid(depths_sorted, mlps_sorted, indexing='xy')  # D: depth (X), M: mlp (Y)\n",
    "\n",
    "    # Fill Z with accuracy\n",
    "    Z = np.zeros_like(D, dtype=float)\n",
    "    lookup = {(int(r.depth), float(r.mlp_ratio)): float(r.accuracy) for r in df_noise.itertuples()}\n",
    "\n",
    "    for i, d in enumerate(depths_sorted):\n",
    "        for j, m in enumerate(mlps_sorted):\n",
    "            Z[j, i] = lookup.get((d, m), np.nan)  # rows correspond to mlp index (j), cols to depth (i)\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(D, M, Z, edgecolor='none', alpha=0.85)\n",
    "    ax.set_title(f\"Accuracy Surface (train_noise={noise_value})\")\n",
    "    ax.set_xlabel(\"Depth\")\n",
    "    ax.set_ylabel(\"MLP Ratio\")\n",
    "    ax.set_zlabel(\"Accuracy (%)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate per noise value and plot\n",
    "for nv in sorted(df[\"train_noise\"].unique()):\n",
    "    plot_surface_for_noise(df[df[\"train_noise\"] == nv], nv)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
